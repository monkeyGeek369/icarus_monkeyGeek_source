# java常见知识点

## 面向对象编程

- 四大特性

  封装:private(只能类内部访问)、protected(同包类+不同包子类)、public

  继承:extends关键字,子类可继承父属性和方法,可重写父方法,单继承

  多肽:重载、重写、向上转型

  抽象:抽象类、接口

- 重写和重载的区别及应用场景

  方法重载(同一个类方法名相同且参数不同[类型/个数/顺序])

  方法重写(子类重写父类方法,可改变返回值[类型范围更窄或子类]和访问级别[可见范围只能更大])

  向上转型(子类转型为父类,子类自己的属性和方法并没丢只是使用时不可见,向下转型是恢复)

- 抽象类和接口的区别及应用场景

  抽象类:abstract,一定包含抽象方法,可以有普通方法,只能被继承

  接口:可以被多实现

- 构造方法的执行顺序

  父类静态代码块(在类首次加载时只执行一次)

  子类静态代码块(在类首次加载时只执行一次)

  父类实例代码块

  父类构造方法

  子类实例代码块

  子类构造方法



## 集合框架

- list接口

  - ArrayList:动态数组实现(扩容时创建更大数组[1.5倍]并将数据复制),访问效率高,时间复杂度o(1),中间插入/删除效率低o(n),内存浪费、线程不安全.适合读多写少的场景.
- LinkedList:底层双向链表,每个元素都包含(数据,前项索引,后项索引),中间插入删除高效o(1)随机访问效率低o(n),内存开销大,线程不安全.适合写多读少场景
  - Vector/Stack:底层与arraylist相同,但线程安全,使用synchronized修饰

- set接口

  - HashSet:底层为数组+链表/红黑树的形式,基于hashmap的实现,底层只关注key的存储.key对象必须有相同的equal和hashcode计算逻辑.当put(key)时对key进行hash计算获取hashcode,利用hashcode与数组长度取模确定桶的位置,然后对桶对应的链表/红黑树的每个元素使用equals判断是否相同,相同则忽略,不同则插入.添加、删除、查找效率高o(1),无序,线程不安全.
- LinkedHashSet:底层借助LinkedHashMap实现,此时的value实际上是占位符,无实际意义
  - TreeSet:底层为红黑树,增删改查的时间复杂度都是o(log n),不允许null,无法排序
- 排序方法:1、自然排序(如果元素实现了Comparable接口则会使用compareTo()排序),2、自定义排序(支持创建时传入自定义比较器Comparator)元素唯一性:使用比较器来实现对比元素唯一性
  
- map接口

  - HashMap:
- 当put(key,value)时对key进行hash计算获取hashcode,利用hashcode与数组长度取模确定桶的位置,然后对桶对应的链表(单向)/红黑树的每个元素使用equals判断是否相同,相同则覆盖value,不同则插入.key为null时哈西为0,不影响计算.
    - 扩容时机:满足“元素个数（size） ≥ 容量(桶的数量) × 负载因子(平均每个桶存放的数量)”时扩容,负载因子一般为0.75(这种设计保证每个桶的元素实际很少,目的是保障hashmap的高效查询/删除/增加为o(1),每个桶的查询保持3个内,查询可忽略不计).扩容操作:1、容量翻倍,2、重新分配所有元素
- LinkedHashMap:可以理解为HashMap+双向链表.所有的数据节点通过HashMap维护,但每个节点比HashMap的节点多了两个数据before、after分别指向前后节点.双向链表以head为头tail为尾.通过双向链表可以快速遍历,通过HashMap来解决哈西冲突和数据存储.
  - Hashtable:键和值都不允许null,使用synchronized标记线程安全.数据结构为数组+单项链表,问题是性能较差.
- ConcurrentHashMap:设计目标是既保证线程安全也保证高并发访问性能,不允许null是因为避免歧义,在并发环境下，`map.get(key)` 返回 `null` 时，你无法判断是这个 key 不存在，还是这个 key 对应的 value 本身就是 `null`
    - java7的分段锁:数据结构为Segment素组,每个Segment元素相当于一个hashmap,每次加锁只针对单个Segment元素加锁.
  - java8及以后:数据结构类似于hashmap,使用“node数组+链表/红黑树”结构,当发生冲突时只使用synchronized对单个桶加锁.内部使用cas(一种乐观锁,即将当前值与期望值比较,如果相同则说明未被变更可以设置新值,否则有变动则不更新)降低枷锁概率,cas操作失败再进行枷锁处理.cas主要应用在“初始化/扩容数组”、“向空桶插入新节点”、“更新特定计数器和状态标志”
    - ABA问题:CAS无法解决ABA问题,在ConcurrentHashMap场景ABA问题并不会造成影响.一般情况ABA问题如何解决?不对值进行对比,而是每次操作都增加版本号,通过对比版本号来判断是否有变化.
- TreeMap:基于红黑树实现的有序集合,非线程安全,不允许null键.利用比较器来判断元素是否相同
  
- 迭代器

  作用:实现了Iterable接口的集合提供统一的方式提供遍历,而无需针对不同的集合使用不同的遍历方式.

  特点:可以安全删除元素.(在删除当前元素的同时，维护迭代器自身的状态，避免抛出异常)

  fail-fast机制:当迭代器在遍历集合的过程中，检测到集合的结构被意外修改（不是通过迭代器自身的修改方法）时，立即抛出 `ConcurrentModificationException` 异常.原理是其内部维护了计数器**`modCount`**,当对象内的元素有变动这个计数器会变化,通过对比原职即可发现.



## 异常处理

- 异常体系结构

  Java 中所有的异常和错误都继承自 `java.lang.Throwable` 类。这个类是异常体系的根。它有两个直接子类：**`Error`** 和 **`Exception`**

  Error:表示系统级错误,由jvm产生,非代码逻辑错误,例如OutOfMemoryError、StackOverflowError、VirtualMachineError

  Exception:表示代码及代码逻辑异常.分为检查型异常(除了 `RuntimeException` 及其子类以外的所有 `Exception` 的子类都属于检查型异常,例如IOException,FileNotFoundException,SQLException,ClassNotFoundException)、非检查型异常(例如ClassNotFoundException,ArrayIndexOutOfBoundsException,IllegalArgumentException,ClassCastException,NumberFormatException)

- 如何自定义异常

  继承自**`Exception`**或者**`RuntimeException`**

  目的是携带更多自定义信息,更方便补货,做一些定制化逻辑,例如加日志



## 核心API

- **`String`:**不可变,保存在字符串常量池中.为什么不可变?保证安全性(防止意外修改)、线程安全(因为不可变)、保证不同的字符串共享常量池中的不同字面量

- **`StringBuilder`:**可变、非线程安全,内部利用char[]实现,单线程下性能高

- **`StringBuffer`**:可变,线程安全,内部使用了char[]实现,利用synchronized加锁

- object类中的方法

  | `toString()`      | 对象字符串表示 | 调试和日志输出             |
  | ----------------- | -------------- | -------------------------- |
  | `equals()`        | 对象相等比较   | 重写时需同时重写hashCode() |
  | `hashCode()`      | 哈希值计算     | 影响哈希集合的性能         |
  | `getClass()`      | 获取运行时类   | 反射的基础                 |
  | `clone()`         | 对象克隆       | 需要实现Cloneable接口      |
  | `wait()/notify()` | 线程同步       | 必须在同步代码块中使用     |

为什么重写equals时要同时重写hashCode?

java规范规定,如果两个对象`equals()`返回true，那么它们的`hashCode()`必须返回相同的值,如果两个对象的`hashCode()`相同，它们不一定`equals()`.如果不同时重写在例如hashmap/hashset中使用hashCode来定位桶,利用equals来匹配,就会出问题.例如<u>HashSet中可能出现"相等"的多个对象</u>

getClass()详细介绍?

不能被子类重写、native方法,用于获取此对象的运行时类.作用是精确的类型判断、反射的基础(获取类内部信息以及实例化对象),单例保证(同一类的class唯一)

clone详细介绍?

必须实现Cloneable接口否则报异常,默认实现浅拷贝(属性是饮用类型职copy了引用地址),深copy需要手动实现引用字段的克隆

wait()/notify()详细介绍?

线程间通信的重要机制,构成了java内置的监视器.必须在同步块synchronized中使用

wait:让当前线程等待，直到其他线程调用notify()或notifyAll().基本流程:释放锁,允许其他线程进入同步块,当前线程进入等待队列,等待超时或被通知,如果被唤醒则重新竞争锁

notify:唤醒一个正在等待的线程



## 基本数据类型

- 八种基本类型及其包装类

  | 基本类型     | 关键字  | 字节数     | 默认值   | 取值范围       | 包装类    |
  | :----------- | :------ | :--------- | :------- | :------------- | :-------- |
  | 字节型       | byte    | 1字节      | 0        | -128 ~ 127     | Byte      |
  | 短整型       | short   | 2字节      | 0        | -32768 ~ 32767 | Short     |
  | 整型         | int     | 4字节      | 0        | -2³¹ ~ 2³¹-1   | Integer   |
  | 长整型       | long    | 8字节      | 0L       | -2⁶³ ~ 2⁶³-1   | Long      |
  | 单精度浮点型 | float   | 4字节      | 0.0f     | ±3.4e38        | Float     |
  | 双精度浮点型 | double  | 8字节      | 0.0d     | ±1.7e308       | Double    |
  | 字符型       | char    | 2字节      | '\u0000' | 0 ~ 65535      | Character |
  | 布尔型       | boolean | 未明确定义 | false    | true/false     | Boolean   |

- 自动拆装箱

  装箱:将基本数据类型自动转换为对应的包装类对象

  拆箱:将包装类对象自动转换为对应的基本数据类型

- 缓存机制

  Java对一定范围内的整型值进行了缓存（默认是-128到127）。当通过自动装箱创建这个范围内的值时，不会创建新的对象，而是返回缓存中的对象

  主要针对 `Integer`, `Byte`, `Short`, `Long`, `Character` 这几个包装类

  例如:Integer缓存-128~127因此如果对象a、b都赋值127则两者a==b返回true

  ==与equals:对于基本类型==比较值,对包装类型比较内存地址.包装与基本比较时先进行拆箱再比较值.equals比较对象值

  三目运算符陷阱:三目运算符要求第二第三操作数类型一致,如果其中有一个是基本类型那么另一个要进行拆箱,当被拆箱对象是null时会报错

- 范型类型擦除

  在编译期间会将所有泛型类型参数移除，并替换为它们的**边界类型**.例如无边界<T>` 会被擦除为 `Object,有边界`<T extends SomeClass & SomeInterface>` 会被擦除为**第一个边界类型**（即 `SomeClass`）,通配符 `<?>`, `<? extends SomeClass>`, `<? super SomeClass>` 也会被擦除为它们的上界或下界

  无法使用基本数据类型作为参数:例如List<int> list = new ArrayList<>(); // 编译错误！

  instanceof 和 getClass() 操作在泛型下失效,原因是范型擦除了,无法得到真实范型

  桥接方法:当一个类继承了一个泛型类或实现了泛型接口时，类型擦除可能会导致子类方法无法覆盖父类方法.此时编译器会自动生成一个**桥接方法**来保证多态.(即生成一个方法来在内部调用子类方法)

- 范型通配符

  表现形式如<?>或者<? extends T>或者<? super T>.通配符虽然更灵活但是在读取和添加时会有更多限制,主要是由于类型擦除后的限制,需要注意.

  ```java
  public void addToList(List<?> list) {
      list.add("hello"); // 编译错误(不能向无界通配符列表中添加除null外的任何元素)
  }
  ```

- PreparedStatement vs Statement

  Statement:用于执行静态 SQL 语句。每次执行都需要将完整的 SQL 字符串发送给数据库,数据库再进行编译和执行.效率低,容易收到sql注入攻击.

  PreparedStatement:使用占位符?来标记参数位置,数据库收到后编译并存储(预编译机制),后续执行只需给到参数即可,效率高,避免sql注入.(参数值会被严格视为**数据**，而不会被再次解析为 SQL 语法)



## 多线程与并发

- 线程创建方式

  - 继承Thread类并重写run()方法
  - 实现runnable接口并重写run()方法,无返回值
  - 实现callable接口并重写call()方法,有返回值(可自定义)
  - 使用线程池

- 线程生命周期

  - new:新建,线程被创建但未启动
  - runnable:可运行,线程正在JVM中执行，但可能正在等待操作系统资源（如CPU）
  - blocked:阻塞,等待监视器锁
  - Waiting:等待,等待被其它线程唤醒.在Object.wait()、Thread.join()时进入
  - Timed_waiting:超时等待,线程在指定的时间内等待
  - terminated:终止,线程执行完毕

- 线程同步与锁

  - synchronized:会阻塞线程,且具有原子性.同步实例方法 - 锁是当前实例对象、同步静态方法 - 锁是当前类的Class对象、同步代码块 - 可指定锁对象(任意对象).底层原理主要依赖于 JVM对 **对象监视器（Monitor）** 的实现.锁升级的过程是无所--偏向锁(只有一个线程反复进入的场景)--轻量级锁(多个线程交替执行的场景,每次线程执行CAS获取锁)--重量级锁(依赖操作系统互斥实现)
  - 对象监视器:每个对象都有与之关联的monitor,锁的本质是对monitor的竞争.java对象包含对象头,对象头包含mark word

  | 锁状态   | Mark Word 内容                                |
  | -------- | --------------------------------------------- |
  | 无锁     | 哈希码 + 分代年龄 + 偏向锁标志（01）          |
  | 偏向锁   | 线程 ID + Epoch + 分代年龄 + 偏向锁标志（01） |
  | 轻量级锁 | 指向栈中锁记录的指针 + 锁标志（00）           |
  | 重量级锁 | 指向 Monitor 对象的指针 + 锁标志（10）        |
  | GC 标记  | 全为 0                                        |

  - AQS:提供一个**基于 FIFO 等待队列 + 状态管理 + CAS 原子操作**的通用同步器框架.
    - 内部维护volatile int state表示状态:含义由子类定义ReentrantLock用作持有次数,Semaphore用作剩余许可数
    - 双向链表（内部称为 CLH 变种）:每个等待线程被封装为node节点
    - 内部分为独占式和共享式两种:可自行实现
    - 如何避免频繁挂起/唤醒?线程入队后先判断前趋节点是否为head,如果是则尝试再获取一次,否则调用park()挂起
    - 如何保证线程安全?state通过volatile+cas保证,队列通过cas+自旋保证
  - ReentrantLock:公平锁(严格按照先来后到执行)/非公平锁(后来的线程通过执行cas有可能插队成功),具有可重入性,实现了lock接口,底层基于AQS实现
  - CAS:乐观锁,CAS(V, A, B)底层基于cpu指令实现,即比较v的值是不是a如果是则赋值b否则有变动,重新循环获取值再次尝试.但是会有ABA问题,可以通过产生版本号来解决.如果竞争激烈cpu等待时间长开销大,jvm会优化对多个cas进行合并或者升级高级并发工具.

- 线程间通信

  - wait()/notify()/notifyAll() 机制:wait()会释放锁，notify()/notifyAll()不会释放锁
    - wait与sleep的区别:前者来自object,会释放锁,后者来自thread,不会释放锁
  - lock.condition接口:可以通过await和signal来进行等待和通知
  - BlockingQueue阻塞队列:空时获取会阻塞,满时增加会阻塞.内部通过可重入锁+CAS实现,不允许存null.实现类有ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue、SynchronousQueue
  - CountDownLatch:内部维护了计数器(初始值由创建时构造函数指定),子线程执行结束后可以调用countDown进行计数器-1,主线程调用await进行阻塞,直到计数器为0时结束.计数器不能被重置.底层基于AQS实现
  - CyclicBarrier:用于让一组线程互相等待，直到所有线程都到达某个公共屏障点（barrier point）后，再一起继续执行.每个线程调用await表示已经到达屏障,当所有线程到达屏障表示屏障被打破,此时所有线程被唤醒继续执行.屏障可重复使用,可用于下次屏障.底层实现基于ReentrantLock + Condition
  - Semaphore:信号量,用于控制同时访问特定资源的线程数量.通过acquire获取许可证,release释放许可证.支持指定是否公平.底层基于AQS实现(错误地多调用 release() 会导致许可证数超过初始值)

- JUC包

  - Volatile:

    - 禁止指令重排序(目的是保证读写操作的顺序性,利用读写屏障实现.[写屏障:确保在 `volatile` 写操作之前的所有读写操作，都不会被重排序到写操作之后],[读屏障:确保在 `volatile` 读操作之后的所有读写操作，都不会被重排序到读操作之前])
    - 确保内存可见性(jvm底层保证A线程在自己的工作线程修改后立即同步到主内存并设置其它线程中的缓存失效,迫使其必须从主线程读取[底层利用缓存一致性协议实现]),不会阻塞线程.问题是其无法实现原子性,因此不能实现计数(例如count++操作并非一个指令)

  - Atomic

    - 原子操作,底层基于CAS实现.是无锁编程和非阻塞编程的基础.**AtomicReference**可以对对象进行原子操作

  - Threadlocal:线程本地局部变量,每个线程都维护有ThreadLocal.ThreadLocalMap threadLocals,当程序中创建后,使用它进行读取/写入实际上是通过线程中的threadLocals来进行读写,这个threadLocals中的key为程序中的对象本身,value为具体值.需要注意key是弱引用,当key被回收value一直存在会造成内存泄露,需要及时调用remove.另外如果配合线程池使用更要及时remove,否则线程池中的线程复用逻辑也会造成影响.

  - InheritableThreadLocal:支持父子线程之间数据共享

  - 并发容器

    - ConcurrentHashMap:见上文
    - CopyOnWriteArrayList:写时复制,读时不加锁.底层使用 `volatile Object[] array` 存储元素保证可见性,所有写操作（如 `add()`、`set()`、`remove()`）会加锁（使用 `ReentrantLock`保证原子性），并**复制一份新数组**，在新数组上完成修改，然后将 `array` 引用指向新数组
    - ConcurrentLinkedQueue:基于无锁算法,底层使用CAS+volatile,数据结构为单向链表,先进先出
    - BlockingQueue:见上文
    - ConcurrentSkipListMap & ConcurrentSkipListSet:线程安全、非阻塞、并发集合.底层基于跳表数据结构实现.读操作不加锁使用volatile保持可见性,写操作使用CAS+synchronized保持原子性.

  - 线程池

    ```java
    ThreadPoolExecutor executor = new ThreadPoolExecutor(
        5,  // 核心线程数：即使空闲也会保留的线程数
        10, // 最大线程数：允许创建的最大线程数
        60, // 空闲线程存活时间,针对非核心线程
        TimeUnit.SECONDS,
        new LinkedBlockingQueue<>(100), // 工作队列容量
        Executors.defaultThreadFactory(), // 线程工厂
        new ThreadPoolExecutor.AbortPolicy() // 拒绝策略
    );
    // 工作队列
    // ArrayBlockingQueue：有界队列，需要指定容量
    // LinkedBlockingQueue：无界队列（默认Integer.MAX_VALUE）
    // SynchronousQueue：不存储元素的队列,立即提交执行
    // PriorityBlockingQueue：具有优先级的无界队列
    
    // 拒绝策略
    // 1. AbortPolicy（默认）：抛出RejectedExecutionException
    // 2. CallerRunsPolicy：由调用者线程执行任务
    // 3. DiscardPolicy：直接丢弃任务
    // 4. DiscardOldestPolicy：丢弃队列中最旧的任务
    ```

    ```java
    // 1. 固定大小线程池
    ExecutorService fixedThreadPool = Executors.newFixedThreadPool(10);
    // 2. 单线程线程池
    ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor();
    // 3. 可缓存线程池(底层为new ThreadPoolExecutor(0,Integer.MAX_VALUE,60L, TimeUnit.SECONDS, new SynchronousQueue<Runnable>());)使用同步队列不等待,直接提交.缓存体现在线程空闲后有60s等待时间接受新任务,如果没有则销毁,这段时间就是缓存
    ExecutorService cachedThreadPool = Executors.newCachedThreadPool();
    // 4. 定时任务线程池(支持定时及周期性任务执行,底层基于ScheduledThreadPoolExecutor)
    ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5);
    ```



## JVM虚拟机

- 内存模型(运行时数据区)
  - 程序计数器:线程私有,字节码行号指示器
  - 虚拟机栈:线程私有,存储局部变量表、操作数栈、动态连接、方法返回地址、一些动态信息
  - 本地方法栈:native方法服务
  - 堆
    - 年轻代:Eden+survivor0+survivor1比例为8:1:1
    - 年老代:YongGC只有在eden满时才触发,回收eden+s0或s1,当对象age年龄计数器达到15放入年老代
    - 永久代/方法区/元数据区:java8之后不属于堆
  - 方法区:多线程共享、可以是不连续的空间、关闭jvm会被释放
    - 存放类信息:名称、加载器、修饰符、方法...
    - 运行时常量池:字面量、符号引用、字符串常量池
    - 静态变量
    - JIT代码缓存
- 垃圾回收算法
  - 标记清除算法:从根节点标记可达对象(非垃圾对象)，对未被标记对象进行清除.适合老年代
  - 复制算法:每次将存活对象复制到另一区域,适合年轻代
  - 标记整理算法:标记存活对象然后让所有存活的对象都向一端移动，直接清理掉端边界以外的内存。适合年老代。
  - 分代回收算法:不同年龄代使用不同算法
  - 增量收集算法:每次垃圾收集线程只收集一部分区域，接着切换到应用线程，以此往复，直到垃圾收集完成。底层依然是标记清除和复制算法。优点减少STW,缺点频繁切换线程
  - 分区收集算法:把一块大的内存空间划分为若干区域的小空间，每一个小空间的分代年龄不同(新生代，老年代，永久代，大对象)，每次只对需要收集的部分区域进行收集。
- 垃圾回收器
  - 串行回收器
    - Serial收集器:年轻代、复制算法、单线程
    - Serial Old收集器:标记整理算法、老年代、单线程
  - 并行回收器
    - ParNew收集器:年轻代、复制算法、多线程
    - Parallel Scavenge收集器:年轻代、复制算法、多线程(它追求的是达到可控的吞吐量)
    - Parallel Old收集器:标记整理算法、老年代、多线程
  - 并发回收器
    - CMS收集器:老年代、标记清除、目的是尽可能缩短STW.初始标记(单线程)、并发标记(与用户线程并发)、重新标记(多线程)、并发清除(与用户线程并发),并发时都不需要STW
    - G1收集器:分区回收算法、将整个堆划分为2048个大小相同的区域,每个分区只可能属于Eden区，幸存者区，老年代区等一个角色,优先处理回收价值大的区域.可预测的停顿,有计划的回收
    - ZGC:分区回收、标记整理算法、目标是在保证吞吐量的前提下实现延迟在10ms内
    - Shenandoah:低延迟
    - Epsilon:只负责内存分配，不负责回收,主要用于虚拟机开发人员调试
- 类加在机制
  - 类加载器:将.class文件 ，加载到JVM内存中，并生成java.lang.Class类的一个实例
    - 类型:引导类-->扩展类-->系统类-->自定义类
    - 双亲委派:某个特定的类加载器在接到加载类的请求时，首先将加载任务委托交给父类加载器，父类加载器又将加载任务向上委托，直到最父类加载器，如果最父类加载器可以完成类加载任务，就成功返回，如果不行就向下传递委托任务，由其子类加载器进行加载.有点是避免重复加载
  - 加载过程
    - 加载:.class文件到class对象
    - 连接:验证(符合JVM要求)、准备(为类变量开辟空间并赋默认值)、解析(符号引用替换为直接引用)
    - 初始化:执行类构造方法clinit的过程,此方法不是我们创建的，是javac编译器自动收集类变量的赋值动作和静态代码块中的语句合并而来



## IO流与NIO

- IO
  - 面向流、阻塞
- NIO
  - 面向缓冲区、同步非阻塞、基于事件驱动
  - 核心组件
    - channel:双向的,可读可写.数据总是从 Channel 读入 Buffer，或从 Buffer 写入 Channel.FileChannel、SocketChannel、ServerSocketChannel、DatagramChannel
    - buffer:每个channel都有自己的buffer,所有数据必须先写到缓冲区再从缓冲区读取.flip()：写模式切换为读模式clear()：清空缓冲区，准备写入compact()：压缩缓冲区，保留未读数据
    - Selector:作用是用一个线程管理多个channel,避免为每个链接都创建线程.底层依赖操作系统多路复用机制如linux下的poll/epoll/select
- AIO
  - 异步非阻塞，基于回调机制,完成后会收到通知



## Spring框架

- IOC&DI
  - 将对象的生命周期交给容器管理无需手动创建管理.两种容器BeanFactory、ApplicationContext(前者的子接口)DI是ioc的一种是实现方式
  - `@Autowired` 根据类型查找`@Resource`优先根据名称查找,找不到在根据类型查找 `@Inject` java提供的标准依赖注入,根据类型查找
  - 容器存储结构:两层结构.获取时先从singletonObjects获取,没有则从BeanDefinition获取并创建实例
    - BeanDefinition 注册表:存储bean的基本信息定义对象private final Map<String, BeanDefinition> beanDefinitionMap = new ConcurrentHashMap<>(256);
    - 单例 Bean 缓存池（实际对象）:private final Map<String, Object> singletonObjects = new ConcurrentHashMap<>(256);
- 反射机制原理:在运行时动态分析、操作类的能力.每个类在JVM中加载后，都会生成一个唯一的`java.lang.Class`对象,获取到class对象就可以实现反射(获取方式类名.class、对象.getClass()、Class.forName()),反射可以突破访问限制符的限制
- bean:被容器管理的对象称为bean,容器管理bean的生命周期.bean的作用域`singleton` (单例)和 `prototype` (原型模式,每次都创建新的)不要将 `prototype` Bean 直接注入到 `singleton` Bean 中（除非使用代理或工厂模式）
  - 实例化:通过反射调用构造方法创建实例
  - 属性赋值:设置了DI的属性支持注入赋值
  - 设置bean名称
  - 设置 BeanFactory / ApplicationContext（可选）:设置容器引用关系
  - BeanPostProcessor 的前置处理（Before Initialization）
  - 初始化回调
  - BeanPostProcessor 的后置处理（After Initialization）
  - bean就绪:可被程序使用
  - 容器关闭时销毁
- AOP
  - 面向切面编程,通知类型.@Before`, `@After`, `@AfterReturning`, `@AfterThrowing`, `@Around
  - 底层基于jdk动态代理(基于接口)或者cglib动态代理(基于类继承)
- springboot:核心是做到了约定大于配置
  - 价值
    - 自动装配:例如发现classpath中有第三方依赖,自动解析为bean并装配好可用
    - 起步依赖:把常用的依赖项汇总成一个springboot依赖即可,避免依赖重复和手动重复引入
    - 内嵌服务:例如内嵌了tomcat服务
    - 生产就绪:提供了健康检查、日志、监控等功能
  - @SpringBootApplication:这是一个组合注解内部包含
    - @SpringBootConfiguration:表示这是一个 Spring Boot 配置类（本质是 `@Configuration`）
    - @EnableAutoConfiguration:根据 classpath 中存在的类、已定义的 Bean、各种属性等条件，自动配置 Spring 应用上下文
    - @ComponentScan:启用组件扫描





# python常见知识点

## 数据类型

- int、float、complex(复数,分为实部+虚部,可以理解为数学上的表达式,例如2+4j)、str、list(支持不同类型值)、tuple(支持不同类型值)、range(起始值,结束值,步长)[表示不可变整数序列]、dict、set(支持不同类型值)、frozenset([1, 2, 3])不可变集合、bool、bytes(字节串例如b'hello')、bytearray(可变字节数组)、memoryview(支持直接操作内存,例如对bytes等对象的操作无需copy)、NoneType
- 可变:list, dict, set, bytearray
- 不可变: int, float, complex, str, tuple, frozenset, bytes



## 函数

- 函数定义

  - 可变参数:def sum_all(*args)表示接收任意数量的位置参数,def print_info(**kwargs)接收任意数量的关键字参数相当于dict
  - 函数作为参数传递:例如def apply_operation(func, x, y)
  - lambda函数:一种匿名函数,格式为“lambda 参数: 表达式”只能是简单的表达式不能包含复杂语句例如print
  - 装饰器

  ```python
  # 方法装饰器:普通
  def my_decorator(func):
      def wrapper():
          print("函数执行前")
          func()
          print("函数执行后")
      return wrapper
  
  @my_decorator
  def say_hello():
      print("Hello!")
  
  # 方法装饰器:带参装饰器
  def repeat(n):
      def decorator(func):
          def wrapper(*args, **kwargs):
              for _ in range(n):
                  result = func(*args, **kwargs)
              return result
          return wrapper
      return decorator
  
  @repeat(3)
  def greet(name):
      print(f"Hello, {name}!")
      
  # 类装饰器
  class CountCalls:
      def __init__(self, func):
          self.func = func
          self.num_calls = 0
      
      def __call__(self, *args, **kwargs):
          self.num_calls += 1
          print(f"调用次数: {self.num_calls}")
          return self.func(*args, **kwargs)
  
  @CountCalls
  def say_hello():
      print("Hello!")
  ```

  - 闭包:**闭包 = 内部函数 + 自由变量 + 外部函数返回内部函数**跟类有点像,装饰器也是一种闭包

  ```python
  def outer(x):
      def inner(y):
          return x + y  # inner 引用了 outer 的参数 x（自由变量）
      return inner      # 返回 inner 函数本身
  # 创建闭包
  closure_func = outer(10)
  # 调用闭包
  print(closure_func(5))  # 输出: 15
  print(closure_func(5))  # 输出: 20
  ```

- 常用内置函数

  - type(100):获取对象类型
  - isinstance(10, int):检查对象类型issubclass(cls, classinfo)判断类是否为另一个的子类
  - hasattr/getattr/setattr/delattr:属性操作
  - 类型转换:str()\int()\float()\bool()
  - abs():绝对值
  - round(3.14159, 2):四舍五入
  - max()/min():最大最小值
  - sum():求和
  - divmod():计算商和余数,例如quotient, remainder = divmod(10, 3)及10被3除,quotient=3,后者1
  - pow():幂运算,pow(2, 3)例如2的3次方
  - sorted()/reversed():排序,例如倒数排序sorted(numbers, reverse=True),反向排序reversed(numbers)
  - enumerate:添加索引,例如for index, fruit in enumerate(fruits)
  - zip():并行迭代,例如for name, age, score in zip(names, ages, scores)
  - range():生成数字序列,例如list(range(1, 10, 2))
  - iter() 和 next() - 迭代器操作
  - all() - 所有元素为真
  - any() - 任一元素为真
  - dir() - 查看对象属性
  - map() - 映射函数:例如result = map(lambda x, y: x + y, a, b)或者int_numbers = list(map(int, str_numbers)),返回的是个迭代器,所以要用list包裹
  - filter() - 过滤序列:例如evens = filter(lambda x: x % 2 == 0, numbers)
  - reduce() - 累积计算:result = reduce(lambda x, y: x + y, numbers)
  - eval() 和 exec():前者只能执行表达式,后者可执行任意python语句
  - globals() 和 locals() :前者获取全局变量字典,后者获取局部变量字典
  - callable() - 检查对象是否可调用
  - help() - 获取帮助信息



## 面向对象编程

- 类内置函数

  - 魔术方法

    ```
    __init__(self, ...)构造方法,负责对类对象赋值
    __str__(self)定义对象的“用户友好”字符串表示（用于 print() 或 str()）
    __repr__(self)定义对象的“官方”字符串表示形式，通常用于调试和开发阶段.例如输出Point(3, 4)
    __len__(self)必须返回非负整数
    __getitem__(self, key)/__setitem__(self, key, value)/__delitem__(self, key)可以自行实现对类属性的操作
    __iter__(self)/__next__(self)允许类自己迭代自己,例如在读取文件流/视频流场景,每次迭代都会读取流但同时也可以自定义记录读取状态,记录状态就是迭代器的作用,而不是记录数据
    __call__(self, ...)使对象可以像函数一样被调用
    __eq__(self, other)、__lt__、__gt__ 等比较方法
    __add__(self, other)、__sub__、__mul__ 等算术方法
    __enter__/__exit__实现上下文管理器（支持 with 语句）用于资源管理（如文件、数据库连接）
    ```

  - 内置方法/工具

    ```
    __new__(cls, *args, **kwargs):负责类对象的新建
    __dict__:用于获取/设置类的内置属性(不包括类方法)
    __class__:返回对象所属的类
    __name__:类的字符串名字
    @classmethod(cls,...)定义类方法,跟着类走不是跟着实例走,第一个参数是类
    @staticmethod(xxx)定义静态方法,不会自动传递类对象
    @property:把方法伪装成“属性”，让调用者不用加括号就能拿到计算后的值，同时保留“只读”或“可拦截赋值/删除”的能力
    super()获取父类cls,注意不是父类对象
    __mro__查看类的继承关系
    ```

- 类实力化过程

  先调用new方法获取对象,再调用init方法赋值

- 方法重写

  子类可以重写父类方法,使用相同方法名称即可,无需特殊标记

- 多继承的MRO

  类可以被多继承,例如class D(B, C)其中BC都继承A,当调用方法时的顺序为D-B-C-A

- 深浅拷贝

  - 使用标准库copy:copy.copy(obj)浅拷贝,copy.deepcopy(obj)深拷贝
  - 自行实现类`__copy__` 和 `__deepcopy__`方法
  - 使用@dataclass实现:被其标记的类支持深浅拷贝方法



## 基础功能

- 异常处理
  - 基本用法是使用try- except,也可以自定义异常类型class MyCustomError(Exception),注意finally总是在函数返回前执行,因此如果其包含return则会覆盖函数的return
- 包与模块
  - 模块即单个py文件,包是包含`__init__.py`文件的目录(其作用为定义版本信息、导入子模块方便用户访问、定义 __all__ 控制 from package import *、包级别初始化代码)
  - 相对导入方式:from . import同级模块from .sibling同级包from .. import父包,from ..parent_package父包模块
  - 模块常用属性:`__name__`名称\`__file__`文件路径\`__doc__`模块文档\`__dict__`模块命名空间,if `__name__` == "`__main__`"
  - 支持动态导入:方式一:import importlib然后importlib.import_module("math"),方式二`__import__`("math")
- 标准库与常用模块
  - os模块:os.getcwd(): 获取当前工作目录os.listdir(path): 列出指定目录下的所有文件和子目录os.mkdir(path) / os.makedirs(path): 创建目录/递归创建多层目录os.remove(path) / os.rmdir(path): 删除文件/删除空目录。os.rename(src, dst): 重命名文件或目录。os.path.join(path1, path2, ...): （非常重要） 智能地连接路径，能正确处理不同操作系统的路径分隔符。os.path.exists(path): 判断路径是否存在。os.path.isfile(path) / os.path.isdir(path): 判断是否为文件/目录。os.path.abspath(path): 获取绝对路径。os.path.basename(path) / os.path.dirname(path): 获取路径的文件名部分/目录名部分。os.path.split(path): 将路径分割为 (目录, 文件名)。os.path.splitext(path): 将路径分割为 (文件名, 扩展名)。
  - sys模块:sys.argv: （非常重要） 命令行参数列表。sys.argv[0]是脚本名，sys.argv[1]及以后是传入的参数。sys.path: Python的模块搜索路径。可以动态修改它来添加自定义模块路径。sys.stdin / sys.stdout / sys.stderr: 标准输入、输出和错误流。sys.exit([arg]): 退出当前程序。sys.version: 获取Python解释器的版本信息。
  - datetime模块:datetime.date: 表示日期（年、月、日）。datetime.time: 表示时间（时、分、秒、微秒）。datetime.datetime: （最常用） 表示日期和时间的组合。datetime.timedelta: （非常重要） 表示两个时间点之间的间隔（时间差）。
  - json模块:json.dumps(obj): 将Python对象（字典、列表等）编码成JSON格式的字符串。json.loads(s): 将JSON格式的字符串解码成Python对象。json.dump(obj, fp): 将Python对象编码后写入一个文件对象。json.load(fp): 从文件对象中读取数据并解码为Python对象。
  - collections模块:namedtuple: 创建有名字的元组，可以通过属性访问元素。deque: 双端队列，适合用于队列和栈，从头部添加和删除元素效率高（appendleft, popleft）。Counter: （非常重要） 简单的计数器，是dict的子类。defaultdict: （非常重要） 带有默认值的字典，在键不存在时返回一个默认值。OrderedDict (在Python 3.7+中，普通dict已保持顺序，但OrderedDict仍有额外方法)。
  - re正则模块:re.match(pattern, string): 从字符串开头开始匹配。re.search(pattern, string): 扫描整个字符串，返回第一个匹配对象。re.findall(pattern, string): 找到所有匹配的子串，并以列表形式返回。re.finditer(pattern, string): 找到所有匹配的子串，并以迭代器形式返回。re.sub(pattern, repl, string): 将匹配的子串替换为另一个字符串。
- 迭代器与生成器
  - 迭代器:需要实现 `__iter__`() 和 `__next__`() 方法
  - 生成器:使用 yield 关键字的特殊函数,支持迭代功能.例如def simple_generator():yield 1 yield 2 yield 3每次调用print(next(gen))则执行一个yield.当需要大量数据计算时为了节省内存可以使用生成器.它属于惰性计算.注意生成器只能使用一次.创建生成器场景,例如gen_exp = (x**2 for x in range(5))        # 惰性计算，节省内存
- 上下文管理器
  - 实现了 `__enter__`() 和 `__exit__`() 方法的对象,可以使用with控制资源的释放
  - 利用contextmanager的上下文管理器:例如from contextlib import contextmanager并使用@contextmanager标记方法
  - with工作原理:调用enter方法.执行代码块,无论如何都会在退出时调用exit方法
- 内存管理
  - 引用计数:通过sys.getrefcount(a)来获取对象a的引用计数(可以理解为在内存中的位置序号)
  - 垃圾回收:del a可以手动删除内存对象,gc.collect()手动触发垃圾回收gc.get_threshold()查看各代回收阀值
  - 内存分配机制:Python对小整数和短字符串有缓存,例如a = 100 b = 100 print(a is b)返回true,另外s1 = "hello" s2 = "hello" print(s1 is s2)返回true(如果字符串包含空格或者标点,则不会缓存)
  - 内存泄漏:1、存在循环引用,2、不断向全局对象(如果是个列表等)添加数据,3、自定义的缓存未设置上限
- 全局解释器锁
  - 一个进程一个解释器锁,为了防止多线程同时执行一个代码.如果要绕过可以使用1多进程、2使用c扩展,3使用其它语言解释器.需要注意GIL不等于线程安全,GIL控制线程交替执行,但是对一些非原子操作,例如count+=1依然会存在问题



## 常用科学计算库

- numpy

  ```python
  # 创建数组
  arr1 = np.array([1, 2, 3, 4, 5])  # 从列表创建
  arr2 = np.zeros((3, 3))# 全零数组
  arr3 = np.ones((2, 4))# 全一数组
  arr4 = np.arange(0, 10, 2)# 类似range
  rangearr5 = np.linspace(0, 1, 5)# 等间距数组
  arr6 = np.random.rand(3, 3)# 随机数组
  # 切片(按照维度)
  arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])例如arr[0, 1]# 基本索引(第一维度的0位置的第二维度的1位置)输出2
  arr[0:2, 1:3]# 切片(第一维度的0和2位置的第二维度的1和3)输出: [[2, 3], [5, 6]]
  mask = arr > 5# 布尔索引arr[mask]输出: [6, 7, 8, 9]
  arr[[0, 2], :]# 花式索引,输出第0行和第2行
  # 形状操作
  arr = np.arange(12)
  arr_3d = arr.reshape(2, 3, 2)# 重塑形状
  flat_arr = arr_2d.flatten()# 展平
  transposed = arr_2d.T# 转置
  # 算术运算(满足广播机制的两个对象)
  print(a + b)    # 逐元素相加(相同位置)
  print(a * b)    # 逐元素相乘(相同位置)
  print(np.dot(a, b))  # 点积(相同位置相乘后每个的相乘结果再相加)
  print(np.sqrt(a))      # 平方根
  print(np.exp(a))       # 指数
  print(np.sin(a))       # 正弦
  # 广播机制:对应维度相等或其中一个维度为1或其中一个维度不存在
  a = np.array([1, 2, 3])       # shape: (3,)可视为(1,3)
  b = np.array([[10], [20]])    # shape: (2, 1)
  		# 满足广播机制,广播后变为(2,3)结果为[[11 12 13],[21 22 23]]
  ```

- pandas

  ```python
  # Series创建
  s = pd.Series([1, 3, 5, np.nan, 6, 8])
  # DataFrame创建
  df = pd.DataFrame({
      'A': 1.0,
      'B': pd.Timestamp('20230101'),
      'C': pd.Series(1, index=list(range(4)), dtype='float32'),
      'D': np.array([3] * 4, dtype='int32'),
      'E': pd.Categorical(["test", "train", "test", "train"]),
      'F': 'foo'
  })
  # 列选择
  df['A']          # 单列
  df[['A', 'B']]   # 多列
  # 行选择
  df[0:3]          # 切片
  df.iloc[0]       # 按位置
  df.loc[0]        # 按标签
  df[(df.A > 0) & (df.B < 0)]     # 多条件
  df[df.E.isin(['test', 'train'])] # isin查询
  # 检测
  df.isnull()           # 检测缺失值
  df.dropna()           # 删除缺失值
  df.fillna(value)      # 填充缺失值
  df.fillna(method='ffill') # 前向填充
  # 排序
  df.sort_values('A')                    # 按列排序
  df.sort_values(['A', 'B'])             # 多列排序
  df.sort_index()                        # 按索引排序
  df.rank()                              # 排名
  # 合并
  pd.concat([df1, df2])                    # 连接
  pd.merge(df1, df2, on='key')             # 合并
  # 连接操作
  pd.merge(left, right, how='inner')       # 内连接
  pd.merge(left, right, how='left')        # 左连接
  pd.merge(left, right, how='right')       # 右连接
  pd.merge(left, right, how='outer')       # 外连接
  ```

- PySpark 的 RDD 转换（map/filter）与行动（count/collect）操作区别

  - 转换操作是构建你的计算逻辑蓝图。当你调用一个转换操作（如 `map` 或 `filter`）时，PySpark并不会立即处理你的数据，它只是将这个操作记录到一个称为**有向无环图（DAG）** 的逻辑计划中.行动操作是“监工”，它下达命令并触发真正的计算.

- pytorch

```python
# 张量运算与自动求导
import torch
# 1. 创建张量并追踪
x = torch.tensor([2.0], requires_grad=True)
w = torch.tensor([1.0], requires_grad=True)
# 2. 前向计算
y = w * x
z = y ** 2
loss = z.mean()
# 3. 自动求导
loss.backward() 
# 4. 查看梯度
print("x.grad:", x.grad) # tensor([2.])
print("w.grad:", w.grad) # tensor([4.])
# 5. ！！！重要：在优化步骤中清零梯度
# w.grad.zero_()
# x.grad.zero_()
```

- TensorFlow

```python
# 张量运算与自动求导
import tensorflow as tf
# 1. 创建变量
x = tf.Variable(3.0)
w = tf.Variable(1.0)
# 2. 在梯度带内进行前向计算
with tf.GradientTape() as tape:
    y = w * x
    z = y ** 2
    loss = tf.reduce_mean(z)
# 3. 自动求导
dl_dx, dl_dw = tape.gradient(loss, [x, w])
print("dl_dx:", dl_dx) # tf.Tensor(6.0, shape=(), dtype=float32)
print("dl_dw:", dl_dw) # tf.Tensor(18.0, shape=(), dtype=float32)
# 4. 对于常量，需要使用 watch
a = tf.constant(3.0)
with tf.GradientTape() as tape:
    tape.watch(a)  # 手动监视常量
    y = a ** 2
dy_da = tape.gradient(y, a)
print("dy_da:", dy_da) # tf.Tensor(6.0, shape=(), dtype=float32)
```







## 多线程与并发

- threading模块

  ```python
  # 方式一
  def my_task(name, duration):
      pass
  t1 = threading.Thread(target=my_task, args=("A", 2))
  t2 = threading.Thread(target=my_task, args=("B", 1))
  # 启动线程
  t1.start()#会启动一个新的操作系统线程，并在新线程中自动调用 run() 方法(只是一个普通的方法。如果直接调用 t.run()，它会在当前线程中执行)
  t2.start()
  # 等待线程结束
  t1.join()
  t2.join()
  # 方式二:直接继承
  class MyThread(threading.Thread)
  ```

- 线程同步机制

  ```python
  # 普通锁
  lock = threading.Lock()
  def increment():
      with lock:  # 自动获取和释放锁
          counter += 1
  # 可重入锁
  rlock = threading.RLock()#内部维护一个 持有者线程标识 和一个 计数器（递归深度）
  def recursive_function(n):
      with rlock:
          if n > 0:
              recursive_function(n - 1)  # 递归调用
  # 条件变量:用于复杂的线程间通信，让一个线程等待特定条件成立，而另一个线程在条件成立时通知它。它内部封装了一个锁
  condition = threading.Condition()
  def producer():
      while True:
          with condition:
              if len(queue) == MAX_ITEMS:
                  condition.wait()  # 释放锁，并等待通知
              queue.append(item)
              condition.notify()  # 通知等待的消费者
          time.sleep(random.random())
  def consumer():
      while True:
          with condition:
              if not queue:
                  condition.wait()  # 释放锁，并等待通知
              item = queue.pop(0)
              condition.notify()  # 通知等待的生产者
          time.sleep(random.random())
  # 信号量
  semaphore = threading.Semaphore(3)  # 最多允许3个线程同时访问
  def access_resource(thread_id):
      with semaphore:
          pass
  # Event:一个线程发送“事件”信号，一个或多个其他线程等待这个事件。基于条件变量实现
  event = threading.Event()
  def waiter():
      event.wait()  # 阻塞，直到 event 被 set
  def setter():
      event.set()
  # 守护线程:用于后台支持性任务，如心跳检查、日志刷新、监控等.随着主线程退出而退出
  t = threading.Thread(target=background_task, daemon=True)
  # 线程间通信
  queue.Queue：这是线程间通信最安全、最常用的方式。它已经内置了所有必需的锁逻辑
  local_data = threading.local() # 线程本地局部变量
  ```

- 线程池

  ```python
  from concurrent.futures import ThreadPoolExecutor,as_completed, wait, FIRST_COMPLETED
  with ThreadPoolExecutor(max_workers=2,thread_name_prefix="worker") as executor:
      future1 = executor.submit(task, 2)# 单个提交(第二个参数为task方法参数)
      futures = [executor.submit(task, i) for i in range(5)]# 批量提交
      results = list(executor.map(task, [1, 2, 3, 4, 5]))# 使用map
      # future的使用
      done, not_done = wait(futures, return_when=FIRST_COMPLETED)#等待第一个完成的任务
      future.add_done_callback(callback) # 支持添加回调函数
      result = future.result(timeout=5)  # 5秒超时
  ```

- multiprocessing模块

  ```python
  import multiprocessing
  # 普通创建
  p = multiprocessing.Process(target=worker, args=(f'worker-{i}',))
  # 进程池
  with multiprocessing.Pool(processes=4) as pool:
    results_map = pool.map(square, range(10))# 方法一：map - 阻塞式，按顺序返回结果
    results_async = [pool.apply_async(square, (i,)) for i in range(10, 15)]# 方法二：apply_async - 非阻塞式，返回AsyncResult对象
    result_map_async = pool.map_async(square, range(15, 20))# 方法三：map_async - 非阻塞式的map
  # 进程间通信
  q = multiprocessing.Queue() # 创建队列,q.put(obj) 和 q.get() 是主要方法。
  parent_conn, child_conn = multiprocessing.Pipe() # 创建管道两端,例如父发送消息,子接受消息.默认双向管道,两端都能收发
  num = multiprocessing.Value('i', 0)# 共享内存(需要自己解决同步问题) 'i' 表示整数类型，
  arr = multiprocessing.Array('d', [1.0, 2.0, 3.0])#共享内存(需要自己解决同步问题)'d' 表示双精度浮点数
  # 处理同步问题
  lock = multiprocessing.Lock()
  ```

- asyncio模块

  ```python
  # 基本概念
  同步：代码顺序执行，遇到阻塞会一直等待。异步：遇到阻塞时，事件循环会挂起当前任务，去执行其他可运行的任务，等阻塞结束后再回来继续执行。并发：系统具有处理多个任务的能力，不一定要同时。并行：系统同时执行多个任务（需要多核 CPU）。
  # 协程
  协程是 asyncio 的基本执行单元，是比线程更轻量级的“用户态线程”await只能在协程内部使用,否则无效
  # 事件循环
  负责对协程的管理,例如执行、挂起、通知等,底层依赖操作系统IO多路复用机制,如poll、epoll.注意每个线程最多只能关联一个事件循环，且不同线程拥有各自独立的事件循环.
  # await关键字
  后面必须跟一个 Awaitable 对象（如协程、Task、Future）,会挂起当前协程，直到其后的 Awaitable 对象执行完毕并返回结果。只能在 async def 函数内部使用。
  # Future
  Future 是一个 低级 Awaitable 对象，代表一个异步操作的 最终结果。Task 是 Future 的子类，专门用于包装协程。
  # 同步:异步线程安全
  lock = asyncio.Lock()例如 async with lock:
  semaphore = asyncio.Semaphore(5)例如 async with semaphore:
  event = asyncio.Event()例如await event.wait()
  # 其它注意事项
  1、绝对不要直接在协程中调用同步阻塞函数（如 time.sleep, requests.get），这会 阻塞整个事件循环可以使用线程池
  2、忘记 await 会导致只创建一个协程对象而不是执行，并且该协程不会被执行
  ```

- 异步上下文管理

  ```python
  # 普通方式
  import asyncio
  class AsyncDatabaseConnection:
      async def __aenter__(self):
        	pass
      async def __aexit__(self, exc_type, exc_val, exc_tb):
          pass
  async def main():
      async with AsyncDatabaseConnection() as conn:
          pass
  asyncio.run(main())
  # 高级方式
  from contextlib import asynccontextmanager
  @asynccontextmanager
  async def get_database_connection():
    	yield connection # yield 之前的代码是 __aenter__ 的逻辑。yield 的值是 as 子句接收的值。yield 之后的代码是 __aexit__ 的逻辑，被包裹在 finally 块中以确保执行。
  # async with工作过程
  创建异步上下文管理器AsyncDatabaseConnection,执行aenter并等待完成,执行with内代码,推出时一定执行aexit
  ```

  

## 经典问题

- 死锁问题:核心是多个线程争抢两个锁,a线程获取锁1,b线程获取锁2,导致相互等待.如何解决?设置超时事件、使用可重入锁(避免同一线程获取同一锁导致的死锁)、使用条件变量condition、使用信号量
- 生产者消费者模式:
  - 方式一:使用`queue.Queue`作为安全队列(内置了锁机制)q.put(item)生产q.get()获取,当获取值为none是终止
  - 方式二:利用`threading.Lock` + `threading.Condition`）实现,基本逻辑是put或者get时先获取threading.Lock锁,然后进行数据处理,再通过threading.Condition进行条件通知,生产者和消费者往复
  - 方式三:可以用两个信号量分别表示“空位数量”和“物品数量”生产者：empty_slots.acquire() -> 放物品 -> filled_slots.release()消费者：filled_slots.acquire() -> 取物品 -> empty_slots.release()
  - 如何避免生产者生产过快导致内存溢出?可以设置队列最大长度



# 数据结构与算法

## 常见数据结构

- 数组:内存固定区域,特点是读写效率高,缺点是扩容浪费资源

- 跳表

  底层Level 0维护完整的有序列表,上层是从下层提取出的有序子列表,通过从顶层Level 3逐层往下查找,例如查找11:第3层对比1,15,第2层对比,1,9,15,第1层对比9,13,第4层对比9,11最终找到11.使用随机化方式决定新插入的节点有多少层.比红黑树实现简单,并发性好

  Level 3:                1 --------------------------------> 15 

  Level 2:                1 ---------------> 9 ------------> 15 

  Level 1:        1 -----> 5 -----> 9 -----> 13 -----> 15 

  Level 0:  1 → 3 → 5 → 7 → 9 → 11 → 13 → 15 → (null)

- 链表:单向链表、双向链表、在java/pyton中对应的数据结构

- 栈:先入后出,java中的Stack<Integer>和Deque<Integer>,在python中使用list或者collections.deque

- 队列:先入先出,java中的和python中的

- 树

  ```python
  # 满二叉树：除了叶子节点，每个节点都有两个子节点。
  # 完全二叉树：除最后一层外，其他层都是满的，并且最后一层的节点都靠左排列。堆就是一种完全二叉树。
  # 二叉搜索树:对于任意节点，其左子树所有节点的值 小于 该节点的值。其右子树所有节点的值 大于 该节点的值。没有值相等的节点
  # 平衡二叉树:通过旋转操作，保证任意节点的左右子树高度差不超过1。是高度平衡的，查找效率极高，但维护平衡的代价稍高
  # 红黑树:通过着色和旋转规则，保证从根到叶子的最长路径不超过最短路径的2倍
  # 堆:每个节点的值都比子节点的值大/小
  # B树:一个节点可以有多个子节点（超过两个）,所有叶子节点都在同一层,节点内的关键字是有序的
  # B+树:区别B树的是所有数据记录都保存在叶子节点中，并且叶子节点之间通过指针相连，形成一个有序链表(优点是减少磁盘io次数)
  # Tire树:用于检索字符串.根节点不包含字符，除根节点外每个节点只包含一个字符,从根节点到某一节点的路径上的字符连接起来，就是该节点对应的字符串,每个节点的所有子节点包含的字符都不相同
  ```

- 图

  ```python
  定义：由顶点集合和边集合组成的数据结构。
  无向图 vs 有向图：边是否有方向。
  简单图 vs 多重图：是否存在自环或重复边。
  完全图：任意两个顶点之间都有边相连。无向完全图边数为 n(n-1)/2，有向完全图边数为 n(n-1)。
  连通图（无向图）：任意两个顶点都是连通的。
  强连通图（有向图）：任意两个顶点互相可达。
  无向图顶点的度：与该顶点相关联的边的数量。
  有向图顶点的度：入度（指向该顶点的边数）和出度（从该顶点指出的边数）。
  ```

- 哈希表:java/python中的hash表体现



## 基本算法

- 排序算法

  ```python
  冒泡排序:重复地“遍历”待排序序列，一次比较两个相邻元素，如果它们的顺序错误就把它们交换过来。每次遍历都会将当前未排序部分的最大（或最小）元素“浮”到顶端，如同气泡一样
  选择排序:在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕
  插入排序:通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入
  希尔排序:是插入排序的改进版。它通过将原始数组“分割”成多个子序列（通过一个增量序列 gap），分别对这些子序列进行插入排序。然后逐步缩小增量，再次进行排序。当增量减至1时，整个数组被当作一个序列来处理，此时数组已经基本有序，最后一遍插入排序效率很高
  归并排序O(n log n):采用分治法。将已有序的子序列合并，得到完全有序的序列。即先使每个子序列有序，再使子序列段间有序
  快速排序O(n log n):同样采用分治法。它从一个序列中选择一个元素作为“基准”，然后将序列中小于基准的元素都放到基准左边，大于基准的放到右边。然后递归地对左右两个子序列进行快速排序
  堆排序O(n log n):利用堆这种数据结构所设计的一种排序算法。堆是一个近似完全二叉树的结构，并同时满足堆的性质：即父节点的键值总是大于等于（或小于等于）任何一个子节点的键值
  计数排序:将输入的数据值转化为键存储在额外开辟的数组空间中。作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数
  桶排序:是计数排序的升级版。它假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（可能使用别的排序算法或是以递归方式继续使用桶排序）
  ```

- 搜索算法

  ```
  二分搜索:在已排序的数组中，通过不断将搜索区间对半划分来缩小范围，从而高效地找到目标值
  深度搜索、广度搜索、顺序搜索
  ```

- 树的遍历

  - 深度优先:前序遍历：根 -> 左 -> 右 中序遍历：左 -> 根 -> 右 后序遍历：左 -> 右 -> 根
  - 广度优先:按层从上到下、从左到右访问节点。通常使用**队列**辅助实现
  - 左旋:将当前节点 x 的**右孩子 y**提升为父节点，x 成为 y 的左孩子
  - 右旋:将当前节点 y 的**左孩子 x**提升为父节点，y 成为 x 的右孩子

- 图的遍历:深度优先、广度优先

- 回溯:如果发现当前选择无法得到合法解（或不是最优解），就撤销这个选择，回到上一步，尝试其他选择,当满足某个终止条件（比如找到一个完整解、或穷尽所有可能）时停止.

- 动态规划:拆解问题：把一个复杂的大问题，拆解成几个更小的、相似的子问题。解决子问题：把这些小问题的答案记下来（存到数组或表格里）。组合答案：利用已经得到的小问题的答案，来构建大问题的答案。

- 贪心算法:在每一步决策时，都选择当前看起来最优的那个选项（局部最优解），并期望通过这一系列的局部最优选择，能够最终达到全局最优解

- 双指针技巧:在遍历对象（如数组、链表、字符串）时，不使用单个循环变量，而是使用**两个指针**（通常是索引或引用），通过某种**逻辑移动这两个指针**，从而高效地解决问题

- 分治算法:将原问题分解成若干个规模较小、相互独立、与原问题形式相同的子问题。“相互独立”意味着子问题之间没有重叠，这是它与动态规划的一个重要区别。“形式相同”意味着这些子问题可以用同样的方法来解决。递归地求解各个子问题,将各个子问题的解合并，最终得到原问题的解.



# 数据库技术

## mysql数据库

- InnoDB引擎
  - 支持事务、行级锁、外键、崩溃恢复(redo log)、聚簇索引、缓冲池(最近最少使用算法)
  - **InnoDB 如何处理死锁？** 有一个后台线程**死锁检测**机制，当检测到死锁时，会**回滚其中一个代价最小的事务**，让另一个事务继续执行
- 数据类型:- 整数：TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT- 小数：FLOAT、DOUBLE、DECIMAL- 位值：BIT- 短文本：CHAR、VARCHAR- 长文本：TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT- 二进制：BINARY、VARBINARY- 二进制大对象：TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB- DATE、TIME、DATETIME、TIMESTAMP、YEAR- JSON提供json处理工具,可以对某字段做索引- 空间数据类型(存储点线面的位置数据)GEOMETRY、POINT、LINESTRING、POLYGON等- 集合类型SET、ENUM
- 事务
  - 原子性、一致性、隔离性、持久性
  - 隔离级别:读未提交(脏读)、读已提交(不可重复读)、可重复读(幻读)默认隔离级别、串行化
- 索引
  - **聚簇索引**：索引的叶子节点直接存放**完整的数据行**。一张表**只能有一个**聚簇索引（因为数据行只能按一种方式物理存储）。InnoDB 中，主键就是聚簇索引。如果没有主键，InnoDB 会选择一个唯一的非空索引代替，如果也没有，则会隐式创建一个主键。
  - **非聚簇索引（二级索引）**：索引的叶子节点存储的是**主键值**，而不是数据行本身。查询时需要通过主键值回到聚簇索引（主键索引）中再次查找，这个过程称为**回表**。
  - 最左前缀原则:对于复合索引 (col1, col2, col3)，它相当于建立了 (col1)、(col1, col2)、(col1, col2, col3) 三个索引。
  - 索引失效场景:对索引列使用函数/表达式、索引列参与运算、隐式类型转换、使用 != 或 <>、NOT IN、NOT EXISTS 等否定条件、like以通配符开头、未遵循最左前缀原则、使用 `OR` 且部分条件无索引、索引列上使用 `IN` 且列表过大



## sql语言

- 常见语法

  ```sql
  SELECT DISTINCT department FROM employees;
  SELECT * FROM employees ORDER BY salary DESC, hire_date ASC;
  SELECT e.name, d.department_name FROM employees e INNER JOIN departments d ON e.department_id = d.id; -- inner join只返回两张表都匹配的数据行 left join 返回左表（FROM 后面的表）的所有行，即使右表中没有匹配的行
  SELECT e.name, d.department_name, p.project_name FROM employees e JOIN departments d ON e.department_id = d.id -- JOIN 默认等同于 INNER JOIN
  SELECT * FROM table1 CROSS JOIN table2;-- 笛卡尔积查询(总数=表一行数*表二行数)
  SELECT * FROM table1, table2;-- 隐式笛卡尔积查询
  INSERT INTO employees (name, department, salary) VALUES (),()
  SELECT * FROM employees ORDER BY id LIMIT 10 OFFSET 20;  -- 第3页，每页10条
  CREATE INDEX idx_salary_department ON employees(salary, department);
  ```

- 查询优化:合理创建索引、避免索引失效场景、避免select *、优化join查询、子查询优化、使用explain分析、进行分表分库读写分离

  

## redis

### 为什么说redis能够快速执行

i. 绝大部分请求是纯粹的内存操作（非常快速）

ii. 采用单线程,避免了不必要的上下文切换和竞争条件

iii. 非阻塞IO - IO多路复用



### Redis中的五种数据结构

- **string (字符串)**

redis是使用C语言开发，但C中并没有字符串类型，只能使用指针或符数组的形式表示一个字符串，所以redis设计了一种简单动态字符串(SDS[Simple Dynamic String])作为底实现：

定义SDS对象，此对象中包含三个属性：

1. len buf中已经占有的长度(表示此字符串的实际长度)所以取字符串的长度的时间复杂度为O(1)
2. free buf中未使用的缓冲区长度
3. buf[] 实际保存字符串数据的地方

空间分配原则：当len小于IMB（1024*1024）时增加字符串分配空间大小为原来的2倍，当len大于等于1M时每次分配 额外多分配1M的空间。

- **list (列表)**

在3.2版本之前，列表是使用ziplist和linkedlist实现的。

列表对象使用ziplist编码有如下条件：

1. 列表对象保存的所有字符串元素的长度都小于64字节
2. 列表对象保存的元素数量小于512个
3. 当有任一条件 不满足时将会进行一次转码，使用linkedlist。

***ziplist的结构\***

由表头和N个entry节点和压缩列表尾部标识符zlend组成的一个连续的内存块。

***linkedlist的结构\***

双向链表，插入和删除效率很高，查询的效率却是O(n)[n为元素的个数]。

***quicklist结构\***

3.2后引入它整体宏观上就是一个链表结构，只不过每个节点都是以压缩列表ziplist的结构保存着数据，而每个ziplist又可以包含多个entry。

- **set (集合)**

通过散列表（hashtable）来保证自已存储的每个字符串都是各不相同的值(这些散列表只有键，但没有与键相关联的值)集合是无序的

- **hash (哈希)**

hash底层的数据结构实现有两种：

ziplist实现

当存储的数据超过配置的阀值时就是转用hashtable的结构。同时满足以下两个条件时才会使用这种结构：
当键的个数小于hash-max-ziplist-entries（默认512）
当所有值都小于hash-max-ziplist-value（默认64）

hashtable实现

这种结构的时间复杂度为O(1)，但是会消耗比较多的内存空间。

- **zset (有序集合)**

它的存储方式也有两种：

1. ziplist结构与上面的hash中的ziplist类似，member和score顺序存放并按score的顺序排列
2. skiplist与dict的结合，skiplist用来保障有序性和访问查找性能，dict就用来存储元素信息，并且dict的访问时间复杂度为O(1)

### Redis的持久化

- **RDB持久化**

RDB持久化即通过创建快照（压缩的二进制文件）的方式进行持久化，全量数据,默认方式。体积小,恢复速度快,无法实时、兼容性差

- **AOF持久化**

AOF（Append-Only-File）持久化即记录所有变更数据库状态的指令，以append的形式追加保存到AOF文件中。支持实时、体积大

- **RDB、AOF混合持久化**

该方案的优点是充分利用了RDB加载快、备份文件小及AOF尽可能不丢数据的特性。

- **Redis 持久化方案的建议**

通常的设计思路是利用主从复制机制来弥补持久化时性能上的影响。即Master上RDB、AOF都不做，保证Master的读写性能，而Slave上则同时开启RDB和AOF来进行持久化，保证数据的安全性。

- **Redis 持久化方案的优缺点**



### 缓存穿透、缓存击穿、缓存雪崩解决方案

- **缓存穿透**

指查询一个一定不存在的数据

解决方案：

i. 查询返回的数据为空，仍把这个空结果进行缓存，但过期时间会比较短

ii. 布隆过滤器：将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对 DB 的查询。

- **缓存击穿**

对于设置了过期时间的 key，缓存在某个时间点过期的时候，恰好这时间点对这个 Key 有大量的并发请求过来，这些请求发现缓存过期一般都会从后端 DB 加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把 DB 压垮。

解决方案：

i. 使用互斥锁：当缓存失效时，不立即去 load db，先使用如 Redis 的 setnx 去设置一个互斥锁，当操作成功返回时再进行 load db 的操作并回设缓存，否则重试 get 缓存的方法。

ii 永远不过期：物理不过期，但逻辑过期（后台异步线程去刷新）。

- **缓存雪崩**

设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效

解决方案：将缓存失效时间分散开

### Redis 的集群模式

- **主从复制**

当从数据库启动时，会向主数据库发送sync命令，主数据库接收到sync后开始在后台保存快照rdb，在保存快照期间收到的命令缓存起来，当快照完成时，主数据库会将快照和缓存的命令一块发送给从数据库。复制初始化结束。 之后，主每收到1个命令就同步发送给从。

- **哨兵模式**

哨兵的作用：1、监控redis主、从数据库是否正常运行2、主出现故障自动将从数据库转换为主数据库。

哨兵的核心知识

1、哨兵至少需要 3 个实例，来保证自己的健壮性。

2、哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。

3、对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。

4、配置哨兵监控一个系统时，只需要配置其监控主数据库即可，哨兵会自动发现所有复制该主数据库的从数据库。

### Redis分布式锁

先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。如果在setnx之后执行expire之前进程意外crash或者要重启维护了，这个锁就永远得不到释放了，使用set指令把setnx和expire合成一条指令来用

### 一些问题

- **内存淘汰机制**（当内存不足以容纳新写入数据时）

1. noeviction: 新写入操作直接报错
2. allkeys-lru：移除最近最少使用的 key
3. allkeys-random：随机移除某个 key
4. volatile-lru：在设置了过期时间的键空间中，移除最近最少使用的 key
5. volatile-random：在设置了过期时间的键空间中，随机移除某个 key
6. volatile-ttl：在设置了过期时间的键空间中，有更早过期时间的 key 优先移除

- **Redis 和 Mysql 的数据不一致怎么办**

  ***延时双删\***

  - 第一步：先删除缓存（为了清除老数据）
  - 第二步：再写入数据库
  - 第三步：休眠xxx毫秒（避免立即删除缓存导致的其他同步线程将脏数据刷入）
  - 第四步：再次删除缓存（为了清除其他线程增加的脏数据）

  ***异步更新缓存+重试策略\***

  - 基于mysql binlog进行数据库更新分析

- **Redis常见性能问题和解决方案**

Master最好不要做任何持久化工作.主从复制不要用图状结构，用单向链表结构更为稳定，即：Master <- Slave1 <- Slave2 <- Slave3…

- **mySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据**

1、如果数据呈现幂律分布，也就是一部分数据访问频率高，一部分数据访问频率低，则使用allkeys-lru

2、如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用allkeys-random

- **Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？**

使用keys指令可以扫出指定模式的key列表

对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。

- **项目中有没有用Redis事务**

采用的是Redis Cluster集群架构，不同的key是有可能分配在不同的Redis节点上的，在这种情况下Redis的事务机制是不生效的。其次，Redis事务不支持回滚操作，所以基本不用



# 软件设计模式

## 创建型

- 单例
- 工厂方法:可以理解为抽象类+抽象方法
- 抽象工厂:可以理解为接口+接口方法定义,只提供口子,不提供具体实现类
- 建造者模式:核心思想是将对象的构建分步进行,可以理解为java中的lombok的builder功能
- 原型模式:就是copy,可以理解为java



## 结构型

- 适配器:它不改变原有类，而是通过一个“中间人”（适配器）把旧接口“翻译”成新接口，让原本无法合作的双方顺利协作。在系统集成、遗留代码改造、第三方库封装等场景中非常实用
- 装饰器:python中的wrapper就是装饰器的一种
- 代理模式:java中的aop就是代理的一种
- 外观模式:隐藏系统的复杂性，只向客户暴露一个简化了的接口,将需要复杂多步骤的功能暴露为简单的统一接口.这种设计模式最常见
- 桥接模式:**识别出系统中独立变化的维度，并通过组合的方式将它们连接起来**，而不是通过继承将它们耦合在一起。这种"组合优于继承"的思想是现代面向对象设计的重要原则之一.这种设计模式很常见.例如某个类包含多个属性,每个属性都是代表不同作用的其它实现类.
- 组合模式:**用一致的方式处理树形结构中的叶子节点和非叶子节点**，让客户端无需关心处理的是单个对象还是对象组合("组合"体现在：**一个对象里面包含了其他同类型的对象**，形成了一种"自我引用"的结构)
- 享元模式:找出大量相似对象中的不变部分（内在状态）进行共享，而变化的部分（外在状态）在使用时传入.可以理解为类某些属性可以共享,另一些属性需要传入.



## 行为型

- 观察者模式: 一个对象（被观察者）的状态发生变化时，会自动通知所有依赖它的对象（观察者）
- 策略模式:定义一系列算法，把它们一个个封装起来，并且使它们可以相互替换(相互替换的含义是**在运行时，客户端可以动态地更换使用不同的具体策略对象，而无需修改上下文（Context）的代码**。这种“互相替换”是通过**面向接口编程 + 组合（而非继承）** 实现的)
- 模板方法模式:定义一个操作中的算法骨架，而将一些步骤延迟到子类中实现
- 责任链模式:**我不知道谁该处理这个请求，但我可以把请求交给第一个对象，让它决定是否处理，或者转给下一个**从而避免请求的发送者与接收者之间的**耦合**
- 命令模式: 将“请求”封装成一个对象，从而使你可以用不同的请求对客户进行参数化,将发出请求的对象和执行请求的对象解耦，支持撤销、排队、日志记录等操作.
- 状态模式:**状态不同，行为不同；状态切换，行为自动切换**,把每个状态的行为封装成类，状态变了，行为自动变，对象“像换了个人.业务上在不同的节点传递不同的状态类
- 迭代器模式:提供一种方法顺序访问一个聚合对象中的各个元素，而又不暴露该对象的内部表示
- 中介者模式: 用一个中介对象来封装一系列对象之间的交互
- 访问者模式:将作用于某种数据结构中的各元素的操作分离出来，封装成独立的类.解决的问题:需要在不同但相关的对象上执行一些不相关的操作，同时避免这些操作“污染”这些对象的类
- 备忘录模式:在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后可以将该对象恢复到原先保存的状态