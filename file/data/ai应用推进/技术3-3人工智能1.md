# LLM应用技术栈

### RAG检索增强生成

- RAG 技术原理与核心组件
  
  - 技术原理:一种将信息检索（Retrieval）与语言生成（Generation）相结合的混合架构，用于提升大语言模型（LLM）在特定领域或私有知识上的回答准确性与可信度。其核心思想是：在生成答案前，先从外部知识库中检索与用户问题最相关的文档片段，再将这些片段作为上下文输入给 LLM，引导其生成更准确、可溯源的回答
- 核心组件
  1. 知识库（Knowledge Base）
     - 存储结构化或非结构化文本（如政策文件、FAQ、数据库文档等）。
     - 通常为向量数据库（如 FAISS、Milvus、Pinecone、Weaviate）。
  2. 检索器（Retriever）
     - 负责将用户查询（query）转化为向量，并在知识库中进行相似度搜索。
     - 常用方法：稠密检索（Dense Retrieval，如使用 Sentence-BERT）、稀疏检索（如 BM25）或混合检索。
  3. 生成器（Generator）
     - 通常是大语言模型（如 Llama、ChatGLM、Qwen、GPT 等）。
     - 接收“问题 + 检索到的上下文”作为输入，生成自然语言回答。
  4. 向量化模块（Embedding Model）
     - 将文本（查询和文档块）映射为高维向量，用于语义相似度计算。
- 优化策略
  - 查询重写（Query Rewriting）：对原始问题进行改写（如使用 LLM 生成多个变体），提升召回率。
  - 多跳检索（Multi-hop Retrieval）：迭代检索，逐步细化上下文（如 HotpotQA 场景）。
  - 重排序（Re-ranking）：在初步检索后，用更精细的模型（如 Cross-Encoder）对结果重排。
  - 缓存机制：对高频查询缓存检索结果，提升响应速度。
  - 上下文压缩：对检索到的长文本进行摘要或关键句提取，避免 LLM 上下文过载。
- 向量化技术
  - 稠密向量(Dense Embedding）:优点：语义理解强；缺点：计算成本高
  - 稀疏向量(Sparse Embedding）:优点：可解释性强、适合关键词匹配；缺点：语义泛化弱
  - 混合向量(Hybrid Search）:结合稠密与稀疏向量（如 ColBERT + BM25），兼顾语义与关键词匹配
  
  - Chunking（文本分块）策略
| 策略         | 说明                                              | 适用场景                   |
| ------------ | ------------------------------------------------- | -------------------------- |
| 固定长度分块 | 按字符/词数切分（如 512 tokens）                  | 通用，但可能切断语义       |
| 语义分块     | 基于句子边界、段落或语义连贯性（如使用 NLP 工具） | 政策文件、法律条文         |
| 重叠分块     | 相邻块有重叠（如 overlap=50 tokens）              | 防止关键信息被截断         |
| 层级分块     | 先按章节分，再细分段落                            | 结构化文档（如政府白皮书） |
  - Embedding 模型选型
    - 中文场景:BGE（智源）：bge-large-zh、bge-m3（支持多语言+稀疏+稠密）、m3e（MokaAI）：轻量高效、GTE（Alibaba）：GTE-Qwen 系列
    - 通用/多语言:text-embedding-3-large（OpenAI）、E5（Microsoft）、Voyage / Cohere Embed
- 评估指标
  - 检索阶段:Recall@K、MRR（Mean Reciprocal Rank）、Hit Rate
  - 生成阶段: Faithfulness（事实一致性）、Answer Relevance、ROUGE/BLEU（若有人工答案）
  - 端到端:用户满意度（人工评估）、任务完成率
  - 工具：RAGAS、TruLens、ARES
- 典型问题与挑战
  - 检索不相关:优化 Embedding 模型、引入查询扩展,其实还要看问题出在哪个环节
  - 幻觉（Hallucination）:提供词工程优化
  - 上下文过载:重排序 + 上下文压缩
  - 知识更新滞后:建立自动化知识注入流水线
  - 长尾查询效果差:引入 fallback 机制（如通用 LLM 回答 + 标注）
- 在政务系统的应用
  - 应用场景
    - 政策问答机器人：市民咨询“生育补贴申请条件”，系统从最新政策文件中检索并生成答案。
    - 公文辅助撰写：根据历史公文模板和法规，自动生成初稿。
    - 内部知识助手：公务员查询办事流程、法律依据。
    - 舆情回应支持：快速从权威口径库中提取标准回应。
  - 特殊要求
    - 数据安全：知识库需部署在私有云/本地，避免敏感信息外泄。
    - 权威溯源：回答必须标注来源（如“依据《XX条例》第X条”）。
    - 多轮对话管理：结合对话状态跟踪（DST）维持上下文一致性。
- 知识图谱与 RAG 结合（GraphRAG）
  - 什么是 GraphRAG:由微软提出，将知识图谱（KG）结构引入 RAG 流程，利用实体关系增强检索与推理能力。
  - 核心思想
    1. 从原始文档构建知识图谱（实体-关系-实体）。
    2. 用户查询时，先在图谱中定位相关子图（如“某政策 → 适用对象 → 企业”）。
    3. 检索不仅基于文本相似度，还基于图结构（如 PageRank、Personalized PageRank）。
    4. 将子图信息（结构化+文本）作为上下文输入 LLM。
  - 优势
    - 支持多跳推理（如“哪些政策适用于高新技术企业？”）
    - 提升关系理解与逻辑一致性
    - 减少对纯文本语义匹配的依赖
  - 挑战
    - 图谱构建成本高（需 NER、关系抽取）
    - 动态更新复杂
    - 需要图数据库支持（如 Neo4j、Nebula）
- 其它常见常考知识点
  - RAG vs Fine-tuning
    - RAG：适合知识频繁更新、需可解释性的场景；无需训练 LLM。
    - Fine-tuning：适合任务风格固定（如摘要、分类），但知识固化、成本高。
    - 趋势：RAG + LoRA 微调（轻量适配）成为主流。
  - RAG 的三种范式
    1. Naive RAG：检索 → 生成（基础版）
    2. Advanced RAG：查询优化、检索、重排、上下文压缩
    3. Modular RAG：可插拔模块（如工具调用、记忆机制、图检索）
  - 向量数据库选型对比
| 数据库   | 特点                          | 适用场景        |
| -------- | ----------------------------- | --------------- |
| FAISS    | Facebook 开源，纯内存，高性能 | 离线/小规模     |
| Milvus   | 分布式，支持标量过滤          | 中大型政务系统  |
| Pinecone | 托管服务，易用                | 快速原型        |
| Weaviate | 支持混合检索（BM25 + 向量）   | 多模态/混合搜索 |



### Agent框架

- 常见设计架构介绍（面向智能体/Agent系统）
  - ReAct（Reasoning + Acting）
    - 核心思想：交替进行推理（Reason）与行动（Act），通过“Thought → Action → Observation”循环实现任务求解。
    - 适用场景：需要结合外部工具或环境反馈的问题（如问答、任务规划）。
    - 优点：结构清晰、可解释性强。
    - 缺点：依赖高质量的推理提示，错误可能累积。
  - Plan-and-Execute（规划-执行）
    - 核心思想：先由 Planner 制定高层次任务计划（如子任务列表），再由 Executor 逐个执行。
    - 代表框架：BabyAGI、AutoGPT（早期版本）。
    - 优点：适合复杂多步任务，结构模块化。
    - 缺点：规划可能不切实际，缺乏动态调整能力。
  - Multi-Agent 架构
    - 核心思想：多个具有不同角色或能力的 Agent 协同工作（如 Manager、Worker、Reviewer）。
    - 通信方式：消息传递、共享记忆、黑板模型等。
    - 代表系统：MetaGPT、CAMEL、Microsoft AutoGen。
    - 优点：可扩展性强，模拟人类团队协作。
    - 缺点：协调成本高，可能出现死锁或冗余。
  - Reflexion(思考) / Self-Reflection 架构
    - 核心思想：Agent 在执行后进行自我评估，若失败则反思并重试。
    - 关键技术：错误检测 + 自我批评 + 迭代优化。
    - 优点：提升任务成功率，具备“学习”能力。
    - 缺点：增加计算开销，依赖高质量反馈信号。
  - Toolformer / Function Calling 架构
    - 核心思想：大模型通过自然语言调用外部工具（API、数据库、代码解释器等）。
    - 实现方式：结构化输出（如 JSON）触发工具调用。
    - 代表：OpenAI Function Calling、LangChain Tools、LlamaIndex Tool Use。
    - 优点：扩展模型能力边界，实现“感知-行动”闭环。
    - 缺点：工具调用格式需严格对齐，错误处理复杂。
- Workflow 编排
  - 指对多个任务、服务或Agent的执行流程进行有序控制与协调，确保它们按预定逻辑（顺序、并行、条件分支、循环等）协同完成复杂目标
    - 控制多步推理流程
    - 协调多个 Agent 的交互
    - 管理工具调用与状态流转
    - 实现错误恢复与重试机制
  - 主流技术框架及特点

| 框架                    | 技术特点                                                     | 适用场景                          |
| ----------------------- | ------------------------------------------------------------ | --------------------------------- |
| LangChain               | - 提供 Chain、Agent、Tool 等抽象 - 链（Chains）、记忆（Memory）、检索增强生成（RAG）、工具与代理（Tools & Agents）、文档加载与处理 - 内置多种 Agent 类型（ReAct、Plan-and-Execute） - 社区生态丰富 | 快速原型、RAG、简单 Agent 应用    |
| LlamaIndex              | - 专注数据连接与检索增强 - 支持高级查询引擎（如子问题分解、多步查询） - 与 LangChain 可集成 | 知识密集型问答、私有数据接入      |
| Microsoft AutoGen       | - 原生支持 Multi-Agent - 提供 GroupChat、Manager 等编排模式 - 支持人类-in-the-loop - 可自定义 Agent 行为与通信 | 多角色协作、复杂任务分解          |
| Haystack                | - 由 deepset 开发，专注 pipeline 编排 - 节点式 DAG（有向无环图）设计 - 强检索与问答能力 | 企业级 RAG、可解释 pipeline       |
| Prefect / Airflow       | - 通用工作流引擎（非专为 LLM 设计） - 强调度、监控、重试能力 - 可集成 LLM 作为任务节点 | 生产级任务调度、批处理 Agent 任务 |
| Semantic Kernel（微软） | - 插件化设计（Functions as Plugins） - 支持自然语言触发函数 - 内置 Planner（Sequential/Stepwise） | 企业级 Copilot 开发、插件生态     |
| langgraph               | 基于langchain构建支持多智能体编排、循环、条件、状态传递、中断与恢复等.组成结构包括节点（Node）、边（Edge）、状态（State）：整个图共享一个可变的状态对象，所有节点都可以读写它。 | 工作流、多伦协作                  |
  - 如何实现工具调用与规划能力
    - 工具调用（Tool Use / Function Calling）
      - 结构化输出：让 LLM 输出符合预定义 schema 的 JSON（如 `{"name": "search", "args": {"query": "xxx"}}`）。
      - Schema 注入：在 prompt 中描述可用工具及其参数（OpenAI 的 `functions` 字段）。
      - 后处理解析：解析 LLM 输出，匹配工具并执行，将结果作为上下文回传。
    - 规划能力（Planning）
      - Prompt-based Planning：通过提示词让 LLM 生成任务步骤（如 “请列出完成该任务的步骤”）。
      - 分层任务网络（HTN）：将高层目标分解为子任务，递归规划。
      - Search-based Planning：结合状态空间搜索（如 BFS、A）与 LLM 评估。
      - LLM as Planner：使用专用 Planner Agent 生成计划，Executor Agent 执行。
  - 其它常见常考点
    - Agent 的记忆机制
      - 短期记忆：对话上下文（token 窗口限制）
      - 长期记忆：向量数据库（如 FAISS、Chroma）存储历史交互，通过检索增强
      - 记忆压缩：摘要、反思（Reflection）减少冗余
    - Agent 的评估指标
      - 任务成功率（Task Success Rate）
      - 步骤效率（Steps to Completion）
      - 工具调用准确率
      - 幻觉率（Hallucination Rate）
      - 人类偏好评分（Human Preference）
    - 安全与对齐
      - 工具调用权限控制（如禁止删除文件）
      - 输出过滤（敏感信息、越狱检测）
      - 沙箱执行（尤其对代码解释器）
    - Agent 与 RAG 的关系
      - RAG 是 Agent 获取外部知识的一种方式
      - Agent 可主动决定“是否需要检索”、“检索什么”
      - 高级模式：Iterative RAG（多轮检索+推理）
    - 当前挑战
      - 可靠性：LLM 非确定性导致流程不稳定
      - 可扩展性：多 Agent 通信开销大
      - 调试困难：黑盒推理 + 外部工具 = 难以追踪错误
      - 成本控制：长上下文 + 多次调用 = 高 token 消耗
- 状态机模型（State Machine Model）
  - 一种根据当前状态和输入事件，决定下一个状态和输出行为的计算模型。在 Agent 或对话系统中，它用于显式建模任务流程的阶段性与条件转移，确保交互逻辑清晰、可控、可预测。
  - 在Agent系统中的作用
    - 结构化任务流程：将复杂任务拆解为有限状态（如“等待用户输入” → “验证信息” → “调用工具” → “确认结果”）
    - 防止逻辑混乱：避免 LLM 自由生成导致流程跳跃或死循环
    - 支持中断与恢复：用户中途切换话题后，可回退到某个状态继续
    - 便于调试与监控：每个状态可埋点、日志、告警
  - 常见类型
| 类型              | 说明                                           | 适用场景                     |
| ----------------- | ---------------------------------------------- | ---------------------------- |
| 有限状态机（FSM） | 状态数量有限，转移规则明确                     | 表单填写、订单创建、客服流程 |
| 层次状态机（HSM） | 状态可嵌套（如“支付”包含“选择方式”“输入卡号”） | 复杂业务流程（如旅行预订）   |
| 数据驱动状态机    | 状态转移依赖外部数据（如库存、用户权限）       | 电商、金融等强业务依赖场景   |
  - 实现方式（结合 LLM Agent）
    - 显式状态管理：在系统中维护一个 `current_state` 变量，每次用户输入后，根据规则或 LLM 判断下一状态。
    - 状态 + LLM 混合控制:状态机决定“现在该做什么”LLM 负责“如何自然地表达”或“解析用户意图是否匹配当前状态”
- 多轮对话逻辑（Multi-turn Dialogue Management）
  - 核心挑战
    - 上下文理解：指代消解（“它”指什么？）、省略补全（“再加一个”）
    - 目标对齐：用户意图可能中途变更
    - 状态维护：已收集哪些信息？还需哪些？
    - 错误恢复：用户说错、系统误解时如何纠错？
  - 多轮对话逻辑的关键组件
    - 对话状态跟踪（DST, Dialogue State Tracking）
      - 作用：维护当前对话的“信念状态”（Belief State），即系统对用户目标和已提供信息的最佳估计。
      - 形式:通常是一个结构化字典
  - 对话策略（Dialogue Policy）
    - 作用：根据当前状态决定下一步动作（提问、调用工具、确认、结束等）
    - 策略类型
      - 规则策略：if slots["destination"] is None → ask destination
      - 强化学习策略：训练 Agent 最大化任务成功率（学术研究多，工业落地少）
      - LLM as Policy：让 LLM 决定下一步（需约束输出格式）
  - 上下文管理（Context Management）
    - 短期上下文：最近 N 轮对话（受 token 限制）
    - 长期上下文
      - 向量检索：将历史对话嵌入，按需召回
      - 摘要压缩：每轮后生成摘要，替代原始历史
      - 显式记忆：关键信息存入结构化数据库（如用户偏好）
  - 多轮工具调用协同
    - 示例流程：
      1. 用户：“帮我订明天从上海到深圳的机票”
      2. Agent 提取：origin=上海, dest=深圳, date=明天
      3. 调用航班查询工具 → 返回多个选项
      4. Agent：“有以下航班：A. 08:00, B. 12:00，选哪个？”
      5. 用户：“A”
      6. Agent 调用预订工具 → 成功 → 确认

    > 此过程需状态跟踪 + 工具调用 + 多轮澄清协同工作。
- 面试/工程常见考点补充
  - 如何处理用户中途改变意图？
     → 设计“意图切换检测”机制，清空或迁移当前状态。
  - 状态机如何支持并行子任务？
     → 使用并行状态机或子状态机嵌套（如同时处理“支付”和“通知”）。
  - 多轮对话中的 token 超限怎么办？
     → 对话历史截断、上下文压缩（摘要）、关键信息提取、滑动窗口、借助RAG
  - 如何评估多轮对话效果？
     → 任务完成率、平均轮次、用户满意度（CSAT）、状态跟踪准确率。



### 大模型微调Fine-tuning

- 什么是微调（Fine-tuning）
  - 在预训练模型上，使用特定数据对模型参数进行进一步训练，使其更好地适应目标任务
- 全参数微调：更新模型所有参数，效果通常最好，但计算和存储开销大。
- 参数高效微调:仅更新少量参数，适用于资源受限场景。常见方法包括：
  - LoRA（Low-Rank Adaptation）：在权重矩阵旁路添加低秩矩阵，训练时只更新低秩部分。
  - Adapter：在Transformer层中插入小型神经网络模块（Adapter模块），只训练这些模块。
  - Prefix Tuning / Prompt Tuning：在输入前添加可学习的连续提示（soft prompts），冻结原始模型参数。
  - IA³（Infused Adapter by Inhibiting and Amplifying Inner Activations）：通过可学习的缩放向量对中间激活进行调制。
- 核心调参技巧
  - 微调阶段
    - 学习率：核心参数，小参数量模型（7B）建议 5e-5~2e-4，大参数量模型（70B）建议 1e-5~5e-5；政务小样本微调需降至 1e-5，避免过拟合；
    - 批次大小（Batch Size）：16GB 显存建议 8~16，32GB 显存建议 32~64；通过梯度累积（accumulation_steps=4）模拟大批次训练，提升收敛效果；
    - 训练轮次（Epochs）：小样本数据（≤1000 条）建议 3~5 轮，配合早停机制（验证集指标连续 3 轮不提升则停止）。
  - LoRA 轻量化微调
    - 秩（Rank）：控制低秩矩阵维度，建议 8~64，Rank=16 时 7B 模型显存需求约 10GB；政务场景优先选 16~32，平衡效果与成本；
    - Alpha（缩放因子）：通常设为 Rank 的 2 倍（如 Rank=16→Alpha=32），增强 LoRA 参数对模型的影响权重；
    - 目标层：聚焦 Transformer 注意力层（q_proj、v_proj），避免微调全部层，进一步降低算力消耗。
  - 推理阶段
    - 温度（Temperature）：控制输出随机性，政务问答场景建议 0.1~0.3（输出更确定），宣传文案生成建议 0.7~0.9；
    - Top-P/Top-K：限制生成词汇范围，Top-P=0.8~0.9、Top-K=50~100，避免输出无关内容；
    - 最大生成长度：政务问答≤512 字符，政策摘要≤1024 字符，避免冗余输出影响服务效率。
- 微调中的过拟合问题如何缓解？
  - 使用较小的学习率；
  - 早停（Early Stopping）；
  - 数据增强；
  - 正则化（如Dropout、权重衰减）；
  - 采用PEFT方法（参数少，天然抗过拟合）；
  - 使用验证集监控性能。
- 人工评估 vs 自动评估的优劣？
  - 自动评估：高效、可复现，但与人类判断相关性有限（尤其在生成任务中）；
  - 人工评估：更可靠，能评估流畅性、事实性、有用性等维度，但成本高、主观性强。
- 评估中的“污染”问题（Data Contamination）
  - 指测试数据在模型预训练或微调阶段已被“见过”，导致评估结果虚高。解决方法：严格划分数据集；
- 为什么现在更倾向于使用LoRA等PEFT方法而非全参数微调？
   - 大模型参数量巨大（如7B、70B），全参数微调需要大量GPU显存和存储（每个任务需保存完整模型副本）。而LoRA等方法仅训练少量参数（通常<1%），显著降低资源消耗，同时保持接近全微调的性能，适合多任务部署和快速迭代。
- 政务场景出现数据稀缺如何解决?
  - 小样本学习技术
    - 提示词工程：设计政务专属 Prompt 模板，如 “请基于《上海市小微企业减税政策（2024 版）》回答问题：{用户提问}。要求：1. 引用条款编号；2. 语言简洁；3. 仅回答政策范围内内容。”，无需微调即可提升效果；
    - Few-Shot Prompting：在提示词中加入 3-5 个政务示例（如 “示例 1：问题：XX；政策条款：XX；回答：XX”），引导模型学习政务应答逻辑；
    - 元学习适配：采用 MAML（模型无关元学习）算法，通过少量跨领域政务样本（如社保→医保咨询）训练模型 “快速学习能力”，降低新场景适配成本。
  - 数据增强技术
    - 文本扰动增强：对政务问答数据进行同义词替换（“审批”→“核准”）、句式转换（主动句→被动句）、关键词增补（“减税”→“小微企业减税”），将 500 条样本扩充至 2000 条；
    - 跨领域迁移：利用国家层面通用政务数据（如《民法典》相关条款）辅助训练地方模型（如上海市政务模型），通过领域适配层减少数据分布差异。



### 多轮对话状态管理与记忆机制设计

- 多轮对话状态管理
  - 核心目标:准确理解用户在当前轮次的意图，并基于整个对话历史，维护一个结构化的、能够精确代表当前对话所处阶段的“状态”表示。这个状态是系统进行下一步决策（如调用哪个工具、如何回复）的直接依据。
  - 关键实现方法
    - 基于框架的槽填充
       - 思想：预先定义一个“框架”，其中包含完成特定任务所需的一系列“槽位”。系统的目标就是通过与用户多轮交互，填满所有必需的槽位。
       - 状态表示：一个字典或JSON对象，键是槽位名，值是已填充的内容。还包括一个“对话动作”（如 `inform`, `request`, `confirm`）
       - 场景：任务导向型对话，如订票、查询、客服。非常结构化，但灵活性较差。
    - 基于链式提示的隐式管理
      - 思想：不维护一个明确的结构化状态，而是将整个对话历史作为上下文，直接输入给LLM。依靠LLM强大的上下文理解能力，来“隐式”地记住状态。
      - 状态表示：状态隐含在对话历史的Token序列中。
      - 管理方法：
        - Prompt构建：将系统指令、对话历史（user和assistant的交替记录）、当前用户问题，一起构建成一个长的Prompt。
        - LLM推理：LLM基于这个完整的上下文生成回复。它需要自己从历史中推断出当前应该做什么。
      - 适用场景：开放域对话、创作、简单问答。非常灵活，但受限于上下文窗口长度，且状态不明确，难以进行复杂逻辑控制。
    - 基于Agent的规划与反思
      - 将对话视为一个需要规划和执行的任务。状态不再是简单的槽位集合，而是一个更复杂的“任务执行状态”，包括已执行的动作、得到的结果、未来的计划以及遇到的困难
      - 管理方法:
        - 规划：Agent根据用户目标和可用工具，制定一个初步计划。
        - 执行：逐步执行计划中的步骤，调用工具（如API、代码执行器）。
        - 观察与反思：在执行每个步骤后，观察结果，并可能触发“反思”步骤，评估当前进展、检查错误、必要时调整计划。
        - 示例：ReAct, Reflexion 等框架。状态在“思考 -> 行动 -> 观察”的循环中不断演进。
      - 场景：需要多步骤工具调用、复杂问题解决的Agent。功能最强大，但设计和实现也最复杂。
- 记忆机制设计
  - 核心目标：长期、高效地存储、压缩、检索和利用对话和交互过程中产生的所有相关信息。它解决了“上下文窗口有限”和“需要长期记忆”之间的矛盾。
  - 短期记忆 / 对话历史缓冲区
     - 实现：简单地将最近N轮对话的原始记录保存在一个列表中（如FIFO队列）。
     - 优点：实现简单，保真度高。
     - 缺点：受限于上下文窗口，无法记忆遥远的过去。Token消耗大，成本高。
     - 变体：摘要式记忆。当对话轮数超过一定限制时，触发一个摘要动作，让LLM对之前的对话历史进行总结，然后用这个摘要来代替原始历史，腾出上下文空间。
  - 长期记忆 / 向量数据库记忆
     - 思想：将对话中重要的信息（如用户透露的个人信息、关键事实、决策结论）转化为向量嵌入，存储到向量数据库中。当需要时，通过语义相似度检索来召回相关记忆。
     - 实现流程：
       - 记忆写入：在对话过程中，识别并提取需要长期记忆的语句或事实（可由规则或另一个LLM判断）。将其转换为文本片段，并生成向量，存入向量DB。
       - 记忆读取：当处理新问题时，将当前问题或对话上下文作为查询向量，在向量DB中进行相似度搜索，召回最相关的K个记忆片段。
       - 上下文增强：将这些检索到的记忆片段作为附加上下文，与当前的对话历史一起送给LLM，辅助其生成更精准、个性化的回复。
     - 优点：突破了上下文窗口的长度限制，可以实现真正意义上的长期记忆和个性化。检索效率高。
     - 缺点：设计“写”和“读”的策略比较复杂，可能存在检索不准确或记忆冲突的问题。
  - 层次化/结构化记忆
     - 思想：模仿人类的记忆结构，将记忆分为不同的类型和层次，以便更精细地管理。
     - 常见分类：
       - 情景记忆：关于特定事件或对话片段的记忆（“我们上周三讨论过AI安全”）。
       - 语义记忆：关于世界的事实和知识（“巴黎是法国的首都”）。
       - 程序记忆：关于如何做事的记忆（“用户A喜欢用图表来展示数据”）。
     - 实现：可以为每种记忆类型建立不同的存储和检索策略。例如，用户偏好用键值对存储，事实知识用向量数据库存储。
  - 记忆的反思与压缩
     - 这不再是简单的存储，而是对记忆的“加工”。
     - 反思：定期或在特定时刻，让Agent回顾最近的经历和记忆，提炼出更高层次的见解、教训或更新核心信念。例如，“我发现用户最近经常问及成本问题，可能他对预算比较敏感。”
     - 压缩：将一系列具体的观察或行动，压缩成一条更抽象的记忆。例如，将“搜索了A产品、搜索了B产品、比较了A和B”这一系列动作，压缩为“用户完成了产品调研阶段”。
- 如何协同工作
  1. 记忆作为状态的输入：当处理新用户输入时，系统会从长期记忆中检索相关历史信息，并将其与最近的对话历史（短期记忆） 一起，作为理解当前问题和更新对话状态的上下文。
  2. 状态触发记忆更新：在对话状态演进过程中，当达到某个里程碑（如一个子任务完成、用户提供了关键个人信息），系统会决定将这些信息写入长期记忆。
  3. Agent的反思作为连接二者的高级活动，它既会审视当前的状态和短期记忆，也会生成新的长期记忆来指导未来的行为。
- 设计选择建议：
  - 简单聊天机器人：优先使用链式提示+对话历史缓冲区。
  - 任务型对话系统（如订餐、查询）：使用基于框架的槽填充，可辅以短期记忆缓冲区。
  - 复杂自主Agent（如AI助手、游戏NPC）：必须结合基于Agent的规划与反思和长期记忆（向量数据库），构建一个层次化的记忆和状态管理系统。



